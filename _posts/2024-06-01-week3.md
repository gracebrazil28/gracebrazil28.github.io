---
layout: post
title: Week 3
---

## My First TMK Model
The first task assigned to me is to create a drawio TMK model for the Means-Ends Analysis skill, specifically for Problem 2, where the student is asked to identify the number of differences from a given state to the goal state. My draft TMK model is below:
During the IVY Team sync-up, I received feedback to break down the method even more and explicitly decompose it to the simplest operator possible. We were also given a point of contact to ask for help to ensure the model uses precise language and semantics, aiming to standardize all project models. Dr. Goel emphasizes consistency in terminology across models, explaining that branding and tradition influence which papers are successful, not just scientific merit. Additionally, the IVY team is working on standardizing the cognitive models to natural language to facilitate smoother integration into the chain of thought for generative AI.

![image](https://github.com/gracebrazil28/gracebrazil28.github.io/blob/85cbfe22b81dde7113575e42f3bb79c3dcc269d3/images/BlockProblem2.png)

### Other DILab Activities
In addition to the weekly meeting with the IVY team, the whole DILab Group also meets every week to discuss and share our opinions, thoughts, and comments on various expository papers around generative AI and how we can possibly use it for our research projects. This week, we've discussed how LLMs like ChatGPT work. Last week, I read about how the transformer works in the context of LLMs, and one of the presenters, who is also one of my DI Labmates, talked about how transformer-based LLMs became so much more successful for tasks like text generation and how they evolved from early classification tasks. Next week, we will be talking about transformers in detail and have Chapter 11: Transformers assigned for reading based on the textbook ***"Deep Learning: Foundations and Concepts"*** by Christopher Bishop, which can be found [here](https://www.bishopbook.com). 


## DREAM Meeting with Mentor and Cohort
This week, I met with my DREAM Mentor, Dr. Tom Williams, head of the [MIRROR Lab](https://mirrorlab.mines.edu/) in the CU School of Mines' [Computer Science Department](https://cs.mines.edu). I was excited after reading about his research interests, which span computer science, robotics, cognitive science, social science, and art history. I had briefly read about his research on robot morality entitled ***"Developing a Quantification System for Robot Moral Agency
"***. I found it particularly interesting that a computer scientist explores such questions. We discussed the background he seeks in prospective PhD students and the diverse career outcomes they can achieve, including academia, research, and venture capital. Despite traveling and facing connectivity issues, I was able to ask about the lab's research and the role of PhD students. Although I had trouble joining the DREAM cohort meeting, I reviewed the video recoding of the meeting and was pleased to see the students' openness to networking. I look forward to the next meeting and joining the slack group that will be created soon.

## Literature Review on Learning Theory

## Literature Review on Mechanistic Interpretability
In the section titled "Mechanistic Interpretability" under the broader category of "Global Explanation" in the paper "Explainability for Large Language Models: A Survey" by H. Zhao et al., I encountered an exploration of how individual neurons in a neural network are examined and their connections to each neuron are understood in terms of circuits. Essentially, the paper views a neural network model as a collection of functional components. This perspective resonates with me due to my background in electrical engineering, where I often analyze concepts such as voltage, current, and resistance, and consider components that utilize these concepts to understand electrical systems from a functional standpoint. While transformer architectures differ significantly from traditional electrical circuits, making connections between the two can be challenging. But by examining neural networks from the perspective of functional sub-components, such as individual layers of attention heads, we can begin to see how they can be similar. For example, viewing one layer as responsible for the copy mechanism and another for deduction, we can see both attention layers as the foundation of in-context learning. One difference lies in the contrast between digital circuits, which tend to be more deterministic, and neural networks, which are characterized by their elasticity and versatility.

In further exploration of this topic, I came across a [detailed demonstration](https://transformer-circuits.pub/2023/monosemantic-features/index.html)  by Anthropic of a sparse autoencoder, which compellingly succeeded in extracting interpretable features from superposition and enabling basic circuit analysis. This approach sheds light on several intriguing aspects of mechanistic interpretability. For example, neurons are described as polysemantic, meaning they respond to mixtures of seemingly unrelated inputs. This phenomenon arises due to superposition, where the neural network represents more independent features of the data by assigning each feature its own linear combinations of neurons, presumably to pack more features into the limited number of neurons available. 

Another interesting point raised is the idea that if we view each feature as a vector over the neurons, then the set of features forms an overcomplete linear basis for the activations of the network neurons. This notion introduces the concept of basis vectors, where any vector in a space can be represented as a linear combination of the basis vectors. When this set of vectors is "overcomplete," it contains more vectors than necessary to span the space. This overcompleteness offers pros such as redundancy and robustness to noise but also presents cons such as issues of interpretation and transparency.

## Other Reading
