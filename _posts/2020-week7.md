---
layout: post
title: Week 7
---

## TMK Modeling Progress

## DILab Meeting and DILab Summer Reading
In our IVY Project meeting, we started our agenda with the NSF Feedback on our IVY Project. NSF evaluators praised IV project's AI and human AI interaction, with recommendations focused on education and learning. Dr. Goal suggests understanding educational goals, learning needs, pedagogical strategies, and instructional design to improve AI-assisted learning. Next, it was discussed that the IVY project needs to be reorganized due to Rochan's departure and Jeanette's potential inability to join as a full-time research scientist. In designing interactive exercises for IVY, we look to the perspective of a teacher, they may ask student to provide solution to problem and critique it, they may also identifies common misconceptions and test the students on it. Dr. Rugaber highlights importance of distinguishing misconceptions from missteps in teaching. Our post doc Rahul  adds that the approach is not limited to tackling student mistakes, but also includes answering a range of questions students might have about a particular skill. These were identified in order for us to better create a learning platform that are aligned with well known instructional designs in education. The rest of the time were spent on evaluating TMK models using various metrics and standardizing the process. Finally, the group plans to reorganize the IVY branches in MCM to simplify the infrastructure and improve the development process.

One of my labmates Samanita presented the paper "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" by the Google Deepmind Team and Jason Wei et al. The main idea is that using chain of thought prompting improves language model performance in solving complex problems by mimicking human reasoning. Moreover, the paper shows that chain of thought prompting can significantly improve language model performance, especially for more complicated problems. With CoT, the model can break down complex problems into steps and arrive at an answer using natural language prompts. For example, CoT prompting outperformed an unaided human sports enthusiast on sports-related questions, demonstrating the potential value of this technique for improving language model performance. Samanita also presented that CoT can be effective for complex problems with multi-step reasoning, but model scale and prompt quality matter. During the discussion, we quickly touched upon transfer learning in machine learning and cognitive science, with a focus on the idea of derivational trace. Dr. Ashok highlights the importance of transferring knowledge from one problem to another and mentions derivational trace. Derivational trace, an idea from the 1980s, is mentioned as a way to make transfer work effectively, involving the steps taken to solve a problem. At the end, it was interesting to note that CoT and prompt engineering did not really help planning tasks. It is because of the nature of planning, which emphasized the importance of understanding relationships between different goals and the interdependence between sub-goals.

## DREAM Cohort Meet-Up and Presentation
This week, I got to meet with the DREAM Cohort Team where we discussed the steps to pursuing a PhD. We discussed the importance of contacting the faculty that we are interested to join to gauge interest and availability. This can help potential students avoid unnecessary application expenses if the faculty is not accepting new students. Next, we discussed the financial challenges faced by prospective students, such as application fees and other costs. Then, the conversation covered the importance of exploring funding options and grants to support these expenses. Finally, finding the right research fit was highlighted and the importance of aligning interests with potential advisors. At the end, Kathleen mentioned plans for another session in late July or early August, focusing on industry versus academia research. I am thankful for these discussions because these are the things that I also take into consideration. Starting a PhD journey takes a lot of time, effort and dare I say, financial risks. The oppurtunity costs are blatant especially in the field of Computer Science, where one can attain a bachelors degree (in our case, Masters) and bag a six figure income, begin saving for retirement and live with financial security. I think that there are many highly driven and very intelligent students that are dissuaded from this path because of this exact predicament. However today, I rest my case. 

## Literature Review on Chain of Thought
Piggybacking on the Summer DILab discussion, I read the paper further and with a focus on common sense reasoning and symbolic reasoning. 

## Literature Review on ICAP Theory

## Other Reading
