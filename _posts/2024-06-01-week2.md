---
layout: post
title: Week 2
---

## Literature Review on Learning Theory
Continuing where I've left off, for each mode of learning, there is an associated knowledge change process that can be expected cognitively. Starting from the lowest level in ICAP, in passive learning, we expect for new information or skill to be stored. _For now, I will refer to knowledge and skill synonymously._ When a new skill is stored, it is characterized to be isolated when it can only be retrieved if the same and specific context is given. So this means that, when a question is rehashed with a different context, the learner is unable to retrieve the new skill needed to solve the problem. In the next mode, active learning, the new skill is integrated with prior experience. Now, the learner is able to relate the skill with prior experience and is able to work with it in combination to what they have learned before. In the next mode, constructive learning, new technique is generated from the integrated skill that they have learned. Finally, with interactive learning, now the learner has a partner which then help them iteratively learn from each other as both generate new knowledge and skill from the integrated knowledge of both partners. Below is the summary of the knowledge change process of each mode of learning:

* Passive -> New isolated skill is stored
* Active -> New skill is integrated with prior experience
* Constructive -> New knowledge is inferred from integrated knowledge or skill
* Interactive -> Learning iteratively via inference of new knowledge through dialogs

Reflecting on my own educational experiences, I realized that true learning occurred during interactive activities, such as studying with my group or working as an undergraduate TA. The interactiveness and constant updating of my knowledge significantly enhanced my skill acquisition. Therefore, for the IVY Agent, embodying interactive learning will be crucial, enabling each student to infer new knowledge through interaction.

## Background work on TMKL2

// Understanding actual diagram of each elements

// Looking at previous implementation with GAIA

// Looking at a worked model

// Working with DI-IVY Team

## Literature Review on Generative AI
During my literature review, technical background on generative AI (Gen AI) architecture is needed in order for me to understand more advanced topics such as taxonomy of XAI techniques, its challenges and future research agenda. I learned about how generative AI is the next level of AI, coming from AI that classifies to an AI that creates. Additionally, I reviewed several Gen AI methods such as transformers, diffusion models, Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) from the paper titled **"Explainable Generative AI (GenXAI): A Survey, Conceptualization, and Research Agenda"** by J. Schneider.

**Transformer models**, prevalent in LLMs such as GPT models, are known for their flexibility due to their reliance on extensive training data. While transformer implementations can vary, a fundamental structure typically consists of an encoder and a decoder, each processing input tokens. Encoders utilize vector embeddings to extract information from their input data, with embeddings representing tokens in a latent space. Tokens in this sense is a way for a computer to understand and manipulate words which captures human semantic understanding.  Additionally, positional encoding, an important component of attention-based encoding, provides information about the relative positions of tokens in the input sequence, which help capture sequential relationships. Then, these are processed through layers of self-attention mechanisms to be fed into a feedforward network for further processing. Once the encoding phase is complete, the decoder generates the output sequence from the encoded information from the encoder. Decoding involves an autoregressive process where the model predicts one token at a time, based on the previously generated tokens. This process continues until the desired output sequence is generated. 

Meanwhile, **diffusion models** are used by generative AI to create images. Its main aim is to learn to reconstruct noisy data. It operates in two phases: the forward pass, where input distortion happens, and the reverse pass, where the output image is generated.  During the forward pass, a technique called Denoising Diffusion Probabilistic Model (DDPM) acts like a Markov chain. This means that only the current input is important for determining the next one. The model distorts the input by adding noise until it resembles a specific distribution, like a Gaussian distribution, which is commonly used in image generation. In the reverse pass, the model is supposed to generate images that match the true data distribution.

The next two Gen AI methods are more widely used and primarily designed for generative modeling: Variational Autoencoders (VAEs) and Generative adversarial Networks (GANs). Both methods generate new data that resembles a given training datasets, both use the concept of latent space and both primarily learns through a probabilistic framework.

**Variational Autoencoders (VAEs)** aims to generate an output similar to what it was trained on by organizing the data of the input into a hidden space or what they mathematically call 'latent space'. The latent space acts as a blueprint based on the distribution of the data. The latent space representation is explicitly modeled through the encoder-decoder architecture. From here, VAEs learn a probabilistic model of the data; then, the output is generated from this learned distribution called 'sample generation'.

**Generative adversarial Network (GAN)** has two key players: generator and discriminator. The generator creates an output based on a random vector, while the discriminator acts as an adversary, distinguishing between the generated output and real samples from the true data distribution.  GANs implicitly learn the data distribution through this adversarial training process. Additionally in GANs, this latent space is learned implicitly through the generator and discriminator networks. 

## Literature Review on LLM Methods: RAG
It has dawned on me that the Meta Cognitive Model (MCM), a library developed by DI Lab's SAMI Team uses the RAG method to generate explanations and I do not have any background knowledge about it. I read the paper titled **"Retrieval-Augmented Generation for Large Language Models: A Survey"** by Yunfan Gao et.al, giving an overview of what RAG is and how it works.  

**Retrieval-Augmented Generation (RAG)** from its name has three parts: retrieval, augmentation and generation. It aims to enhance LLM output by adding a context to the prompt by retrieving relevant information from an external knowledge base. Historically, RAG initially focused on the inference stage and then began to also permeate in fine-tuning stages in generating output. RAG evolves from Naive RAG to Advanced RAG and finally to Modular RAG. 

The three main steps are indexing, retrieval and generation. Indexing involves splitting the external data into chunks, which are encoded into vectors in a vector database. Next is Retrieval where it retrieves top k chunks which are the most relevant to the query based on semantic similarity. Then finally Generation where the original question and retrieved chunks are put together into LLM to generate the final answer. Advanced RAG has added optimized strategies: pre-retrieval and post retrieval in Step 2. Meanwhile the next evolution, Modular RAG, has implemented a more flexible structure within RAG, adding new modules like Search, Memory, Predict and these modules can interact with one another at any step. In short, Modular RAG goes beyond fixed RAG structures and processes, making it the most flexible and adaptable architecture.

## DREAM Kick-Off Meeting

// DREAM Kick off meeting with all the participants

## Other Reading

