---
layout: post
title: Week 2
---

## DILab Onboarding 
Thankfully one of my labmates, Erin,  helped Tim and I get acquainted with TMK model creation and has shown us the list of all skills that needed a TMK model. She also gave us tips on picking what topics to choose first before going on and doing more complicated models. She also has shown the json representation and touched on repository. 

At our weekly meeting, professor Ashok led the discussion about Googleâ€™s initiative in EdTech, learnLM. In the classical AI point of view, generative AI can be further enhanced when models have deep understanding of its output. In contrast, we can have AI models rely on the probabilistic nature of current transformer based architectures.

## Literature Review on Learning Theory
Continuing where I've left off, for each mode of learning, there is an associated knowledge change process that can be expected cognitively. Starting from the lowest level in ICAP, in passive learning, we expect for new information or skill to be stored. _For now, I will refer to knowledge and skill synonymously._ When a new skill is stored, it is characterized to be isolated when it can only be retrieved if the same and specific context is given. So this means that, when a question is rehashed with a different context, the learner is unable to retrieve the new skill needed to solve the problem. In the next mode, active learning, the new skill is integrated with prior experience. Now, the learner is able to relate the skill with prior experience and is able to work with it in combination to what they have learned before. In the next mode, constructive learning, new technique is generated from the integrated skill that they have learned. Finally, with interactive learning, now the learner has a partner which then help them iteratively learn from each other as both generate new knowledge and skill from the integrated knowledge of both partners. Below is the summary of the knowledge change process of each mode of learning:

* Passive -> New isolated skill is stored
* Active -> New skill is integrated with prior experience
* Constructive -> New knowledge is inferred from integrated knowledge or skill
* Interactive -> Learning iteratively via inference of new knowledge through dialogs

Reflecting on my own educational experiences, I realized that true learning occurred during interactive activities, such as studying with my group or working as an undergraduate TA. The interactiveness and constant updating of my knowledge significantly enhanced my skill acquisition. Therefore, for the IVY Agent, embodying interactive learning will be crucial, enabling each student to infer new knowledge through interaction.

## Background work on TMKL2

One of my labmates, Erin, have shared a working TMK model for a KBAI skill (the course that IVY will be deployed). I have started to look into building my first TMK model with the Raven's Progressive Matrice problem in parallel with Guards and Prisoners problem in Chapter 3, Semantic Representation. I also got access to DI Lab's MCM Repository in Gatech Github Enterprise and started to look through the files and try to familiarize myself with the lab's codebase. 

## Literature Review on Generative AI
During my literature review, technical background on generative AI (Gen AI) architecture is needed in order for me to understand more advanced topics such as taxonomy of XAI techniques, its challenges and future research agenda. I learned about how generative AI is the next level of AI, coming from AI that classifies to an AI that creates. Additionally, I reviewed several Gen AI methods such as transformers, diffusion models, Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) from the paper titled **"Explainable Generative AI (GenXAI): A Survey, Conceptualization, and Research Agenda"** by J. Schneider.

**Transformer models**, prevalent in LLMs such as GPT models, are known for their flexibility due to their reliance on extensive training data. While transformer implementations can vary, a fundamental structure typically consists of an encoder and a decoder, each processing input tokens. Encoders utilize vector embeddings to extract information from their input data, with embeddings representing tokens in a latent space. Tokens in this sense is a way for a computer to understand and manipulate words which captures human semantic understanding.  Additionally, positional encoding, an important component of attention-based encoding, provides information about the relative positions of tokens in the input sequence, which help capture sequential relationships. Then, these are processed through layers of self-attention mechanisms to be fed into a feedforward network for further processing. Once the encoding phase is complete, the decoder generates the output sequence from the encoded information from the encoder. Decoding involves an autoregressive process where the model predicts one token at a time, based on the previously generated tokens. This process continues until the desired output sequence is generated. 

Meanwhile, **diffusion models** are used by generative AI to create images. Its main aim is to learn to reconstruct noisy data. It operates in two phases: the forward pass, where input distortion happens, and the reverse pass, where the output image is generated.  During the forward pass, a technique called Denoising Diffusion Probabilistic Model (DDPM) acts like a Markov chain. This means that only the current input is important for determining the next one. The model distorts the input by adding noise until it resembles a specific distribution, like a Gaussian distribution, which is commonly used in image generation. In the reverse pass, the model is supposed to generate images that match the true data distribution.

The next two Gen AI methods are more widely used and primarily designed for generative modeling: Variational Autoencoders (VAEs) and Generative adversarial Networks (GANs). Both methods generate new data that resembles a given training datasets, both use the concept of latent space and both primarily learns through a probabilistic framework.

**Variational Autoencoders (VAEs)** aims to generate an output similar to what it was trained on by organizing the data of the input into a hidden space or what they mathematically call 'latent space'. The latent space acts as a blueprint based on the distribution of the data. The latent space representation is explicitly modeled through the encoder-decoder architecture. From here, VAEs learn a probabilistic model of the data; then, the output is generated from this learned distribution called 'sample generation'.

**Generative adversarial Network (GAN)** has two key players: generator and discriminator. The generator creates an output based on a random vector, while the discriminator acts as an adversary, distinguishing between the generated output and real samples from the true data distribution.  GANs implicitly learn the data distribution through this adversarial training process. Additionally in GANs, this latent space is learned implicitly through the generator and discriminator networks. 

## Literature Review on LLM Methods: RAG
It has dawned on me that the Meta Cognitive Model (MCM), a library developed by DI Lab's SAMI Team uses the RAG method to generate explanations and I do not have any background knowledge about it. I read the paper titled **"Retrieval-Augmented Generation for Large Language Models: A Survey"** by Yunfan Gao et.al, giving an overview of what RAG is and how it works.  

**Retrieval-Augmented Generation (RAG)** from its name has three parts: retrieval, augmentation and generation. It aims to enhance LLM output by adding a context to the prompt by retrieving relevant information from an external knowledge base. Historically, RAG initially focused on the inference stage and then began to also permeate in fine-tuning stages in generating output. RAG evolves from Naive RAG to Advanced RAG and finally to Modular RAG. 

The three main steps are indexing, retrieval and generation. Indexing involves splitting the external data into chunks, which are encoded into vectors in a vector database. Next is Retrieval where it retrieves top k chunks which are the most relevant to the query based on semantic similarity. Then finally Generation where the original question and retrieved chunks are put together into LLM to generate the final answer. Advanced RAG has added optimized strategies: pre-retrieval and post retrieval in Step 2. Meanwhile the next evolution, Modular RAG, has implemented a more flexible structure within RAG, adding new modules like Search, Memory, Predict and these modules can interact with one another at any step. In short, Modular RAG goes beyond fixed RAG structures and processes, making it the most flexible and adaptable architecture.


## Other Reading
This week, I read the chapter _"What Is Consciousness, and Could Machines Have It?"_ by Dehaene et.al from this book **"Robotics, AI and Humanity"** by von Broun et al. [Open Source textbook link](https://link.springer.com/book/10.1007/978-3-030-54173-6). In this chapter, the authors argue that consciousness can be defined with first sense C1 and second sense C2 of consciousness. C1 pertains to the idea of information processing that is globally available for higher level processing such as storage and retrieval. In C1, they defined consciousness as the selection and holding of an instance (a thought) from a stream of parallel information processing in a probabilistic distribution by the unconscious. The unconscious is referred to C0, where many tasks can be done by AI now like recognition/classification tasks. Reading about the bottle neck in serial processing of conscious thought within humans as shown by experimental results was very interesting. I wonder if our bias for faces is due to the fact that infants needed to recognize their parents' faces as survival instinct and due to this overexposure (and I argue, reinforcement learning), we are better face recognizers than we are at checkers recognizers.

Meanwhile, C2 is the concept of metacognition or  the awareness of how the entity thinks, which is the source of reflection, introspection and error correction. Hallucinations in humans arise from the inability to detect if a thought is generated by own self vs the world. Comparing with hallucinations in AI, hallucinations arise from overconfidence in its output. With this, generative methods like GAN has exhibited AI self correction mechanisms. This lead me to ponder upon what would an Error Related Negativity (ERN) system look like in AI. Additionally, I wonder how can it also use the common currency of how humans establish its own confidence in their own decisions and outputs. What self correcting mechanisms can we model from human cognition that will be helpful to try with AI? 

I'll have to catch up on the book I started reading last week for next week, as I left it back in San Francisco. I'm spending the whole week in Minneapolis until I travel again, so this book is one of few books I've stashed here and there for purposes like this.

