---
layout: post
title: Week 6
---

## TMK Modeling Progress
Re-iterating on TMK v3 on Blocks World from peer feedback, I added the initial validation in the beginning of problem solving to check whether any given block arrangement already matches the goal block arrangement and then followed with validating if the given block arrangement is legal. Adding a 'while loop condition-like" statement feedback came from Erin, so I added these to the FSM transition.

```json
      "transitions": {
        "begin problem solving": {
          "type": "validation",
          "source subtask to target subtask": [["Solve using means ends analysis", "Validate Block Arrangement"]],
          "validation condition": "None of the given block arrangement completely match the goal block arrangement."
        },
        "given arrangement matches the goal": {
            "type": "validation",
            "source subtask to target subtask": [["Solve using means ends analysis", "Success"]],
            "validation condition": "Else"
        },
        "validate block arrangement": {
          "type": "validation",
          "source subtask to target subtask": [["Validate Block Arrangement", "Individual Position Extraction"]],
          "validation condition": ["Each block must occupy a unique position.", 
          "If a block is on the table, it can have at most one block directly on top of it.", 
          "If a block is on top of another block, it should not have more than one block directly above it.",
          "A block can only be moved if it is free, no other blocks are stacked on it."
          ]
        },
         "illegal given block arrangement": {
          "type": "validation",
           "source subtask to target subtask": [["Validate Block Arrangement", "Fail"]]
          "validation condition": "Else"
        },
```
The idea to use "validation condition": "Else" came from IVY TMK for Guards and Prisoners (GPP) in [IVY rebased](https://github.gatech.edu/Dilab/MCM-TMK/blob/ivy_rebased/mcm/TMKs/Ivy/Method.json):

```json
   "the updated state is invalid": {
          "type": "validation",
          "source subtask to target subtask": [["updated state", "reject"]],
          "validation condition": "else"
        }
```

Other things done are changing "datacondition" to simply "condition", changing some nuianced terminologies like "select answer" to "select move" and defining legalities in "validate block arrangement". The inputs and outputs within the method model were made less granular since those were explicit in the task model in order to follow the given condition and the post condition of the sub-task within the transitions. Next, I reviewed the WIP Standardization document and tried to modify the Method model again order to make use of 'makes' and 'requires'. Also from this document, I created the text file that describes the TMK similar to Figure 3 in the TMK Standardization file -- with v1 before peer feedback and v2 after peer feedback.

* [Block World TMK v1 txt file](https://gracebrazil28.github.io/files/BlockWorld_v1.txt)
* [Block World TMK v2 txt file](https://gracebrazil28.github.io/files/BlockWorld_v2.txt)
* [Task Model v4 json file](https://gracebrazil28.github.io/files/Task_Model_BlockWorld2_v4.json)
* [Method Model v4 json file](https://gracebrazil28.github.io/files/Method_Model_BlockWorld2_v4.json) 
* [Knowledge Model v4 json file](https://gracebrazil28.github.io/files/Knowledge_Model_BlockWorld2_v4.json)


Last week, I started translating Block World methods into applicable terms for re-use in Ravens Progressive Matrices (RPM) problem. While that was a nice mental exercise, I got the clarification from our post doc/project lead that I should look into Semantic Networks skill for Guards and Prisoners (GPP) problem next and explore to what extent can a TMK model capturing the skill of semantic network for GPP can be applied to RPM. I plan to pick this up once I wrap up on the BW Means End Analysis TMK modeling.


## Literature Review on Interactive Task Learning

Two weeks ago, Sashank has presented the paper, ***“VAL: Interactive Task Learning with GPT Dialog Parsing”*** by Lawley and MacLellan [(acccess here)](https://arxiv.org/pdf/2310.01627). This paper discussed enabling AI systems to learn tasks through iterative natural language interactions. We reviewed this paper since it implements HTN as its knowledge representation, which is also what we are doing with DILAB’s IVY Agent. I wanted to go in detail now with the methodology and evaluation. 

From the review, I’ve learned that the GPT subroutines are not involved in the overall task learning algorithm and when users want to audit the knowledge base. The GPT subroutines are mostly used in translating user input and symbolic knowledge structures. VAL was intended for LLMs to capitalize on the large language fluency on extracting symbolic information, rather than using it to directly learn the task itself. The authors noted that this can be considered as a hybrid neuro-symbolic systems.

In the evaluation, the researchers implemented a user study to evaluate the user experience and the objective performance of VAL’s components. I found it interesting how the opinions on sessions for each user dictated the success rate of the GPT subroutine, which is pretty much central to any HCI research paper. Other details on this section points to VAL’s usability -- we aim to do the same thing for IVY. In the discussion section of the paper, the section on Theory of the Mind really piqued my interest. It emphasized the importance of knowing the intent of AI agents for humans to trust it as much as how the AI agent needs to know what task the human user intends for it to learn. 


## Other Reading
