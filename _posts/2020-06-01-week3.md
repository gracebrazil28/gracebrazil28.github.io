---
layout: post
title: Week 3
---

## My First TMK Model
The first task assigned to me is to create a drawio TMK model for the Means-Ends Analysis skill, specifically for Problem 2, where the student is asked to identify the number of differences from a given state to the goal state. My draft TMK model is below:
During the IVY Team sync-up, I received feedback to break down the method even more and explicitly decompose it to the simplest operator possible. We were also given a point of contact to ask for help to ensure the model uses precise language and semantics, aiming to standardize all project models. Dr. Goel emphasizes consistency in terminology across models, explaining that branding and tradition influence which papers are successful, not just scientific merit. Additionally, the IVY team is working on standardizing the cognitive models to natural language to facilitate smoother integration into the chain of thought for generative AI.

![image](https://github.com/gracebrazil28/gracebrazil28.github.io/blob/85cbfe22b81dde7113575e42f3bb79c3dcc269d3/images/BlockProblem2.png)

### Other DILab Activities
In addition to the weekly meeting with the IVY team, the whole DILab Group also meets every week to discuss and share our opinions, thoughts, and comments on various expository papers around generative AI and how we can possibly use it for our research projects. This week, we've discussed how LLMs like ChatGPT work. Last week, I read about how the transformer works in the context of LLMs, and one of the presenters, who is also one of my DI Labmates, talked about how transformer-based LLMs became so much more successful for tasks like text generation and how they evolved from early classification tasks. Next week, we will be talking about transformers in detail and have Chapter 11: Transformers assigned for reading based on the textbook ***"Deep Learning: Foundations and Concepts"*** by Christopher Bishop, which can be found [here](https://www.bishopbook.com). 


## DREAM Meeting with Mentor and Cohort
This week, I met with my DREAM Mentor, Dr. Tom Williams, head of the [MIRROR Lab](https://mirrorlab.mines.edu/) in the CU School of Mines' [Computer Science Department](https://cs.mines.edu). I was excited after reading about his research interests, which span computer science, robotics, cognitive science, social science, and art history. I had briefly read about his research on robot morality entitled ***"Developing a Quantification System for Robot Moral Agency
"***. I found it particularly interesting that a computer scientist explores such questions. We discussed the background he seeks in prospective PhD students and the diverse career outcomes they can achieve, including academia, research, and venture capital. Despite traveling and facing connectivity issues, I was able to ask about the lab's research and the role of PhD students. Although I had trouble joining the DREAM cohort meeting, I reviewed the video recoding of the meeting and was pleased to see the students' openness to networking. I look forward to the next meeting and joining the slack group that will be created soon.

## Literature Review on Learning Theory
In the next sections of the paper, assumptions about the ICAP taxonomy and the knowledge change processes are discussed. First, when behaviors are addressed for each learning mode, they are considered relevant to the learning activity, excluding behaviors that are not related. Next, for the intended behavior, overt behaviors are studied while covert processes are not. This is because overt manifestations are non-invasive and covert behaviors can be misleading. Moreover, the authors believe overt behaviors are good approximations of actual engagement levels. Another assumption for the ICAP taxonomy is that engagement activities are independent and orthogonal to instructional tasks. This means students can engage in multiple activities simultaneously, regardless of how they are delivered by the instructor. Finally, these engagement activities may have intermode boundaries, meaning a student may exhibit overt behaviors that are both active and constructive, indicating that mode classification is not rigid.

It is important to note that learning causes change, and in this context, change refers to personal knowledge change in a student rather than changes in behavior, habits, or systemic level changes. For passive learning, the associated knowledge change is 'store,' meaning that knowledge is isolated and accessed only when the same specific cue is given, usually referred to as shallow learning. For active learning, the knowledge change is 'integrate,' where new information is assimilated with other relevant knowledge, filling gaps in existing knowledge and making access to it more frequent. 'Infer' is the next higher knowledge change for constructive learning, where knowledge has more details, is revised when existing knowledge is incorrect, and typically has been reflected on by the student. This stage involves applying reasoning to the newly integrated knowledge. Finally, the highest level is 'co-infer,' where the student applies interactive learning to new knowledge, accumulating new knowledge and incorporating inferences from both partners and themselves.

Reflecting on this literature review has made me consider my own learning experiences from grade school to undergraduate and now graduate school. It has strengthened my belief that learning and co-inferring with a study buddy has been the most effective learning tool for me. I believe that the Autonomous Coaching Agent IVY that we are working on at [DILAB](https://dilab.gatech.edu) will enable us to simulate the interactive learning and knowledge change process of co-inference within the realm of human-computer interaction.

## Literature Review on Mechanistic Interpretability
In the section titled "Mechanistic Interpretability" under the broader category of "Global Explanation" in the paper "Explainability for Large Language Models: A Survey" by H. Zhao et al., I encountered an exploration of how individual neurons in a neural network are examined and their connections to each neuron are understood in terms of circuits. Essentially, the paper views a neural network model as a collection of functional components. This perspective resonates with me due to my background in electrical engineering, where I often analyze concepts such as voltage, current, and resistance, and consider components that utilize these concepts to understand electrical systems from a functional standpoint. Making connections between the electrical circuits and neural networks can be challenging because transformer architectures differ significantly from electrical circuits. But by examining neural networks from the perspective of functional sub-components, such as individual layers of attention heads, I can begin to see how they can be similar. For example, viewing one layer as responsible for the copy mechanism and another for deduction, we can see both attention layers as the foundation of in-context learning. 

In further exploration of this topic, I came across a [detailed demonstration](https://transformer-circuits.pub/2023/monosemantic-features/index.html)  by Anthropic of a sparse autoencoder, which compellingly succeeded in extracting interpretable features from superposition and enabling basic circuit analysis. This approach sheds light on several intriguing aspects of mechanistic interpretability. For example, neurons are described as polysemantic, meaning they respond to mixtures of seemingly unrelated inputs. This phenomenon arises due to superposition, where the neural network represents more independent features of the data by assigning each feature its own linear combinations of neurons, presumably to pack more features into the limited number of neurons available. 

Another interesting point raised is the idea that if we view each feature as a vector over the neurons, then the set of features forms an overcomplete linear basis for the activations of the network neurons. This notion introduces the concept of basis vectors, where any vector in a space can be represented as a linear combination of the basis vectors. When this set of vectors is "overcomplete," it contains more vectors than necessary to span the space. This overcompleteness offers pros such as redundancy and robustness to noise but also presents cons such as issues of interpretation and transparency.

## Other Reading
I recently traveled again, this time from San Francisco to Lisbon, the capital of Portugal. During the long flight, I caught up on my reading from Week 1, titled "How to Read a Book: The Classic Guide to Intelligent Reading" by Adler and Van Doren. This time, I read the chapters on elementary reading and inspectional reading. The chapter on elementary reading introduced the history of how it has been taught throughout history with various philosophies and methodologies. It also discussed how children learn to read, highlighting different prerequisites and milestones for each level. Chapter 3 emphasized the importance of moving beyond elementary reading to achieve higher reading competency. The authors suggested that most people stop at elementary reading, which involves basic and functional reading skills. Analytical reading and comparative argumentation between texts on similar topics are skills that many do not pursue. The authors also emphasized that one must strive to go above and beyond to become a competent reader -- this has become a goal of mine.

Chapter 4, on inspectional reading, offers six tips on skimming or "pre-reading" and explains why having a general idea of a book's content is helpful for deeper understanding later. One tip that resonated with me is checking the table of contents and the index section. Following this advice, I examined the table of contents and gained insights from it. The author sets the foundation for why we need to be competent readers in Part 1 (chapters 1-6), provides a proper guide in Part 2 (chapters 7-14), and explains how to extend this reading habit and tackle challenging books in Part 3 (chapters 15-17). I also checked the index and found that the book discusses many great authors and classic works, as well as contemporary philosophy and pivotal books.
Finally, I tried to identify the chapter which is important to the book's argument about what it means to be a good reader. I believe it is in chapters 10-12: "Coming to Terms," "What's the Proposition," and "The Etiquette of Talking Back." These chapters seem to discuss how the reader interprets the text, understands the author's intentions, and determines whether they agree or disagree. I will revisit these chapters once I get to them. Overall,  chapter 4 has given me the insight not to be afraid to skim or pre-read challenging books. I often avoid books that seem too complex, but now I understand the value of adjusting my reading speed. Skimming for an overview and not fixating on every detail can help me grasp the book's content more effectively.

